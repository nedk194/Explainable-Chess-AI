# CM3203-Explainable-Chess-AI

Chess programming is a well established field in computer science, with a rich history that dates back to some of the earliest experiments in artificial intelligence. From Alan Turing’s 1950s algorithms to modern engines like Stockfish and Leela Chess Zero, chess has long served as a benchmark for developing and testing intelligent systems. Typically, chess engines use a combination of search algorithms and evaluation functions to analyze board positions and select optimal moves. In recent times, chess programming has evolved significantly, with advancements in hardware and particularly the integration of machine learning techniques (Pawar, Prof.Shaila, 2023) engines have significantly increased in strength, surpassing even the best human players.


Although traditional chess engines use search algorithms that are conceptually straightforward, such as a minimax algorithm, the depth and complexity of their evaluation functions often make them difficult for humans to follow or replicate. This means that while their decision making process is transparent in structure, it lacks simulatability. In other words humans cannot easily mentally trace and internalise the reasoning behind a move chosen with typical chess algorithms. As a result these engines suggest strong moves without helping players understand why they are effective, or why one move is preferred over another. Contemporary chess engines are often of limited use for the regular club player that want to improve their understanding of the game (Pálsson & Björnsson, 2024) This gap between human reasoning and AI decision making is a key challenge in creating systems that are both powerful and educational. 


Addressing these challenges requires engines that not only play well but can articulate their strategic reasoning in a form accessible to human learners. To address this gap, this project presents a chess engine designed specifically to explain its decisions in a way that is accessible and educational for beginner players. Rather than simply presenting a chosen move with a numerical evaluation, the system presents an accurate and insightful reasoning behind its decision. This will be achieved through an experimentation process; implementing, testing and evaluating various explainability techniques. The basis of these techniques are based on breaking down its reasoning into human-understandable components of a chess board, such as material, king safety, pawn structure, and mobility. It also introduces multiple engine "personalities" with different strategic preferences, allowing users to compare move choices and understand different perspectives and a contextual explanation style to offer diverse perspectives on move choice.  Through these methods, the engine aims to bridge the interpretability gap by making its thought process both visible and relatable to human learners.


Explainable Artificial Intelligence (XAI) is a growing field that focuses on making the reasoning behind AI decisions more understandable to human users. Unlike traditional "black box" systems that provide outputs without context, explainable AI concentrates on developing AI that can explain the reasons behind its decisions in a way that humans can easily understand (Pálsson & Björnsson, 2024) Explainability is a central goal of this project, as the chess engine is designed not only to suggest strong moves but also to explain them in ways that support learning and strategic understanding. This project will investigate how XAI can be incorporated with a chess engine. The potential applications of this research will be briefly explored, as it may contribute to a broader understanding of how explainability techniques can be applied to rule-based AI systems. 
Project Scope and Objectives


The project’s goal was to build a UCI-compatible chess engine that plays competently and explains its reasoning in human-understandable terms. It includes a custom evaluation function, multiple explanation formats aligned with explainable agency, and a graphical interface for interaction and user testing. The system was designed to be modular for experimentation. While limitations in consistency and clarity remain, the engine successfully meets its core aims and contributes to the field of explainable AI in deterministic environments.

